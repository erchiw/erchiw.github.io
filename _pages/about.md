---
permalink: /
title: "Hello, Friend."
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Ph.D. student at <a href="https://datascience.ucsd.edu/" style="text-decoration:none">Halıcıoğlu Data Science Institute</a>, UC San Diego, where I am fortunate to be advised by Prof. <a href="https://cseweb.ucsd.edu/~yuxiangw/" style="text-decoration:none">Yu-Xiang Wang</a> and to work closely with Prof. <a href="https://sites.google.com/view/yianma/home" style="text-decoration:none">Yi-An Ma.</a>

My research interests lie in differentially private machine learning. Recently, I have being thinking how tecniques from differential privacy can address safety concerns in generative models. 

<br />


Selected Papers <small><span style="color:#888888">(\* denotes equal contribution)</span><small>
======
**Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning**
<br> Erchi Wang, <a href="https://jeremy43.github.io/" style="text-decoration:none">Yuqing Zhu</a>, Yu-Xiang Wang
<br> To appear in International Conference on Machine Learning (ICML) 2025. 
<br> Theory and Practice of Differential Privacy workshop (TPDP) 2025, <span style="color: orange;">oral presentation</span>.

**Purifying Approximate Differential Privacy with Randomized Post-processing**
<br> <a href="https://sites.google.com/view/yingyulin" style="text-decoration:none">Yingyu Lin\*</a>, Erchi Wang\*, Yi-An Ma, Yu-Xiang Wang
<br> <a href="https://arxiv.org/pdf/2503.21071" style="text-decoration:none">Arxiv preprint</a>
<br> Theory and Practice of Differential Privacy workshop (TPDP) 2025, <span style="color: orange;">oral presentation</span>.

<br />



Education
======
**University of California San Diego**
<br>Ph.D. in Data Science
<br>Jul 2024 - Now

**University of California Santa Barbara**
<br>Ph.D. in Statistics 
<br>Sep 2021 - Jun 2024. (Transferred to UCSD)


<br />

Contact
======
erw011@ucsd.edu

<!-- <a href="/files/CV_ErchiWang.pdf" style="text-decoration:none">My résumé.</a> -->
